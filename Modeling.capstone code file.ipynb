{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ada8380-5d0d-45ad-b42d-bc95f665286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic regression w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40d93145-aac8-46f3-9b98-9e041333b083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train shape: (307511, 236)\n",
      "Final test shape: (48744, 236)\n",
      "Validation Accuracy: 0.919337268100743\n",
      "Validation AUC: 0.7501270825810031\n",
      "Validation Log Loss: 0.2486389836633079\n",
      "\n",
      "Cross-validation AUC scores: [0.74204458 0.75116269 0.7471765  0.75109602 0.74183978]\n",
      "Mean CV AUC: 0.7466639105813014\n",
      "\n",
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load datasets\n",
    "# ----------------------------\n",
    "app_train = pd.read_csv(r\"C:\\Users\\sina_\\OneDrive\\Documents\\University of utah cybersecurity\\capstone 2\\application_train.csv\")\n",
    "app_test = pd.read_csv(r\"C:\\Users\\sina_\\OneDrive\\Documents\\University of utah cybersecurity\\capstone 2\\application_test.csv\")\n",
    "bureau = pd.read_csv(r\"C:\\Users\\sina_\\OneDrive\\Documents\\University of utah cybersecurity\\capstone 2\\bureau.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Aggregate bureau features\n",
    "# ----------------------------\n",
    "bureau_agg = bureau.groupby('SK_ID_CURR').agg({\n",
    "    'AMT_CREDIT_SUM': ['mean', 'max', 'sum'],\n",
    "    'DAYS_CREDIT': ['min', 'mean'],\n",
    "    'CREDIT_ACTIVE': lambda x: (x == 'Active').sum(),\n",
    "    'CREDIT_TYPE': 'nunique'\n",
    "})\n",
    "bureau_agg.columns = ['_'.join(col).strip() for col in bureau_agg.columns.values]\n",
    "bureau_agg.reset_index(inplace=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Merge with train/test\n",
    "# ----------------------------\n",
    "train_merged = app_train.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "test_merged  = app_test.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Separate numeric and categorical columns\n",
    "# ----------------------------\n",
    "numeric_cols = train_merged.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = train_merged.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove TARGET from numeric list\n",
    "numeric_cols = [col for col in numeric_cols if col != 'TARGET']\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Fill missing values\n",
    "# ----------------------------\n",
    "# Numeric → median\n",
    "train_merged[numeric_cols] = train_merged[numeric_cols].fillna(train_merged[numeric_cols].median())\n",
    "test_merged[numeric_cols]  = test_merged[numeric_cols].fillna(train_merged[numeric_cols].median())\n",
    "\n",
    "# Categorical → mode\n",
    "for col in categorical_cols:\n",
    "    mode_val = train_merged[col].mode()[0]\n",
    "    train_merged[col] = train_merged[col].fillna(mode_val)\n",
    "    test_merged[col]  = test_merged[col].fillna(mode_val)\n",
    "\n",
    "# ----------------------------\n",
    "# 6. One-hot encode categorical features\n",
    "# ----------------------------\n",
    "train_encoded = pd.get_dummies(train_merged[categorical_cols], drop_first=True)\n",
    "test_encoded  = pd.get_dummies(test_merged[categorical_cols], drop_first=True)\n",
    "\n",
    "# Align columns so train and test match\n",
    "train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Combine numeric + categorical\n",
    "# ----------------------------\n",
    "X = pd.concat([train_merged[numeric_cols], train_encoded], axis=1)\n",
    "y = train_merged['TARGET']\n",
    "X_test = pd.concat([test_merged[numeric_cols], test_encoded], axis=1)\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Scale numeric features only\n",
    "# ----------------------------\n",
    "scaler = StandardScaler()\n",
    "X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "print(\"Final train shape:\", X.shape)\n",
    "print(\"Final test shape:\", X_test.shape)\n",
    "\n",
    "# ----------------------------\n",
    "# 9. Train/Validation Split\n",
    "# ----------------------------\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 10. Logistic Regression Model\n",
    "# ----------------------------\n",
    "model = LogisticRegression(max_iter=5000, solver='saga', C=0.5, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------\n",
    "# 11. Standard Metrics on Validation\n",
    "# ----------------------------\n",
    "y_valid_pred = model.predict(X_valid)\n",
    "y_valid_proba = model.predict_proba(X_valid)[:,1]\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_valid, y_valid_pred))\n",
    "print(\"Validation AUC:\", roc_auc_score(y_valid, y_valid_proba))\n",
    "print(\"Validation Log Loss:\", log_loss(y_valid, y_valid_proba))\n",
    "\n",
    "# ----------------------------\n",
    "# 12. Cross-Validation AUC\n",
    "# ----------------------------\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X, y, cv=skf, scoring='roc_auc')\n",
    "print(\"\\nCross-validation AUC scores:\", cv_scores)\n",
    "print(\"Mean CV AUC:\", cv_scores.mean())\n",
    "\n",
    "# ----------------------------\n",
    "# 13. Kaggle Submission File\n",
    "# ----------------------------\n",
    "test_preds = model.predict_proba(X_test)[:,1]\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'SK_ID_CURR': test_merged['SK_ID_CURR'],\n",
    "    'TARGET': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nSubmission file created: submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d589063-9f05-4ad2-b433-f8f1923316ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "the true benchmark is:\n",
    "\n",
    "Accuracy ≈ 91.9%\n",
    "\n",
    "AUC = 0.5\n",
    "\n",
    "Model beats the benchmark in the metric that matters most here: AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b8e170-f5de-435c-a778-65406ac20c87",
   "metadata": {},
   "source": [
    "## Logistic regression with scaler and targeted interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9a6b84-ff47-4be1-a95b-50c490c53342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline numeric-only -> Accuracy: 0.919272 | AUC: 0.737980\n",
      "Numeric + categorical -> Accuracy: 0.919386 | AUC: 0.750191\n",
      "With targeted interactions -> Accuracy: 0.919370 | AUC: 0.750895\n",
      "\n",
      "Model comparison (sorted by AUC):\n",
      "With targeted interactions   | Acc: 0.919370 | AUC: 0.750895\n",
      "Numeric + categorical        | Acc: 0.919386 | AUC: 0.750191\n",
      "Baseline numeric-only        | Acc: 0.919272 | AUC: 0.737980\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# --- Starting point: build X and y from your merged data ---\n",
    "# ----------------------------\n",
    "# 1. Load datasets\n",
    "# ----------------------------\n",
    "app_train = pd.read_csv(r\"C:\\Users\\sina_\\OneDrive\\Documents\\University of utah cybersecurity\\capstone 2\\application_train.csv\")\n",
    "app_test = pd.read_csv(r\"C:\\Users\\sina_\\OneDrive\\Documents\\University of utah cybersecurity\\capstone 2\\application_test.csv\")\n",
    "bureau = pd.read_csv(r\"C:\\Users\\sina_\\OneDrive\\Documents\\University of utah cybersecurity\\capstone 2\\bureau.csv\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Aggregate bureau features\n",
    "# ----------------------------\n",
    "bureau_agg = bureau.groupby('SK_ID_CURR').agg({\n",
    "    'AMT_CREDIT_SUM': ['mean', 'max', 'sum'],\n",
    "    'DAYS_CREDIT': ['min', 'mean'],\n",
    "    'CREDIT_ACTIVE': lambda x: (x == 'Active').sum(),\n",
    "    'CREDIT_TYPE': 'nunique'\n",
    "})\n",
    "bureau_agg.columns = ['_'.join(col).strip() for col in bureau_agg.columns.values]\n",
    "bureau_agg.reset_index(inplace=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Merge with train/test\n",
    "# ----------------------------\n",
    "train_merged = app_train.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "test_merged  = app_test.merge(bureau_agg, on='SK_ID_CURR', how='left')\n",
    "\n",
    "# If you already have train_merged and test_merged (with bureau aggregations), run the following prep.\n",
    "\n",
    "# Separate columns\n",
    "numeric_cols = train_merged.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_cols = [c for c in numeric_cols if c != 'TARGET']\n",
    "categorical_cols = train_merged.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Impute\n",
    "train_merged[numeric_cols] = train_merged[numeric_cols].fillna(train_merged[numeric_cols].median())\n",
    "test_merged[numeric_cols]  = test_merged[numeric_cols].fillna(train_merged[numeric_cols].median())\n",
    "\n",
    "for c in categorical_cols:\n",
    "    mode_val = train_merged[c].mode()[0]\n",
    "    train_merged[c] = train_merged[c].fillna(mode_val)\n",
    "    test_merged[c]  = test_merged[c].fillna(mode_val)\n",
    "\n",
    "# One-hot encode categorical\n",
    "train_encoded = pd.get_dummies(train_merged[categorical_cols], drop_first=True)\n",
    "test_encoded  = pd.get_dummies(test_merged[categorical_cols], drop_first=True)\n",
    "train_encoded, test_encoded = train_encoded.align(test_encoded, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Feature matrices\n",
    "X_num = train_merged[numeric_cols].copy()\n",
    "X_cat = train_encoded.copy()\n",
    "y = train_merged['TARGET'].copy()\n",
    "\n",
    "X_num_test = test_merged[numeric_cols].copy()\n",
    "X_cat_test = test_encoded.copy()\n",
    "\n",
    "# --- Utility: evaluate a feature matrix with scaling on numeric subset ---\n",
    "def evaluate_logistic(X_df, y_vec, numeric_feature_names, model_kwargs=None, random_state=42):\n",
    "    if model_kwargs is None:\n",
    "        model_kwargs = dict(max_iter=5000, solver='saga', C=0.5, n_jobs=-1)\n",
    "\n",
    "    # Train/valid split (stratified)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_df, y_vec, test_size=0.2, random_state=random_state, stratify=y_vec\n",
    "    )\n",
    "\n",
    "    # Scale numeric columns only\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_valid_scaled = X_valid.copy()\n",
    "    if len(numeric_feature_names) > 0:\n",
    "        X_train_scaled[numeric_feature_names] = scaler.fit_transform(X_train[numeric_feature_names])\n",
    "        X_valid_scaled[numeric_feature_names] = scaler.transform(X_valid[numeric_feature_names])\n",
    "\n",
    "    # Fit logistic regression\n",
    "    model = LogisticRegression(**model_kwargs)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Metrics\n",
    "    y_valid_pred = model.predict(X_valid_scaled)\n",
    "    y_valid_proba = model.predict_proba(X_valid_scaled)[:, 1]\n",
    "    acc = accuracy_score(y_valid, y_valid_pred)\n",
    "    auc = roc_auc_score(y_valid, y_valid_proba)\n",
    "    return acc, auc, model, scaler\n",
    "\n",
    "# --- 1) Baseline numeric-only model ---\n",
    "X_baseline = X_num.copy()\n",
    "acc_base, auc_base, model_base, scaler_base = evaluate_logistic(X_baseline, y, numeric_cols)\n",
    "print(f\"Baseline numeric-only -> Accuracy: {acc_base:.6f} | AUC: {auc_base:.6f}\")\n",
    "\n",
    "# --- 2) Numeric + categorical (full) ---\n",
    "X_full = pd.concat([X_num, X_cat], axis=1)\n",
    "acc_full, auc_full, model_full, scaler_full = evaluate_logistic(X_full, y, numeric_cols)\n",
    "print(f\"Numeric + categorical -> Accuracy: {acc_full:.6f} | AUC: {auc_full:.6f}\")\n",
    "\n",
    "# --- 3) Add targeted interaction terms ---\n",
    "# Choose a small set of informative numeric features for interactions (customize as needed).\n",
    "# Pick stable, non-sparse features to avoid blow-up. Examples (adjust to your column names):\n",
    "selected_numeric_for_interactions = [\n",
    "    c for c in numeric_cols\n",
    "    if c in [\n",
    "        'AMT_CREDIT', 'AMT_ANNUITY', 'AMT_INCOME_TOTAL',\n",
    "        'DAYS_BIRTH', 'DAYS_EMPLOYED',\n",
    "        'AMT_CREDIT_SUM_mean', 'AMT_CREDIT_SUM_sum'\n",
    "    ] and c in X_num.columns\n",
    "]\n",
    "\n",
    "# Build polynomial (interaction-only) features on the selected numeric subset\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_inter_num = X_num[selected_numeric_for_interactions].copy()\n",
    "X_inter_arr = poly.fit_transform(X_inter_num.values)\n",
    "inter_feature_names = poly.get_feature_names_out(selected_numeric_for_interactions)\n",
    "\n",
    "X_inter_df = pd.DataFrame(X_inter_arr, index=X_num.index, columns=inter_feature_names)\n",
    "\n",
    "# Combine: original numeric + interactions + categorical\n",
    "X_with_interactions = pd.concat([X_num, X_cat, X_inter_df], axis=1)\n",
    "\n",
    "# Evaluate\n",
    "# Numeric columns for scaling include the original numeric plus the interaction columns (all are numeric)\n",
    "numeric_cols_with_interactions = list(X_num.columns) + list(X_inter_df.columns)\n",
    "acc_inter, auc_inter, model_inter, scaler_inter = evaluate_logistic(\n",
    "    X_with_interactions, y, numeric_cols_with_interactions\n",
    ")\n",
    "print(f\"With targeted interactions -> Accuracy: {acc_inter:.6f} | AUC: {auc_inter:.6f}\")\n",
    "\n",
    "# --- Optional: simple ranked printout ---\n",
    "results = [1`\n",
    "    (\"Baseline numeric-only\", acc_base, auc_base),\n",
    "    (\"Numeric + categorical\", acc_full, auc_full),\n",
    "    (\"With targeted interactions\", acc_inter, auc_inter),\n",
    "]\n",
    "print(\"\\nModel comparison (sorted by AUC):\")\n",
    "for name, acc, auc in sorted(results, key=lambda r: r[2], reverse=True):\n",
    "    print(f\"{name:28s} | Acc: {acc:.6f} | AUC: {auc:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8f11d-86ae-4456-8479-f5f1346949b1",
   "metadata": {},
   "source": [
    "## Using the baseline Logistic Regression, Random Forest and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed3b9006-a4ac-419c-b474-6fa69ae03fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression       | Accuracy: 0.919272 | AUC: 0.549845\n",
      "Random Forest             | Accuracy: 0.919272 | AUC: 0.742343\n",
      "Gradient Boosting         | Accuracy: 0.919760 | AUC: 0.758034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.919760011706746, 0.7580335011250177)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = pd.concat([X_num, X_cat], axis=1)\n",
    "y = train_merged['TARGET']\n",
    "\n",
    "# --- Use your prepared X (features) and y (TARGET) ---\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_valid, y_valid, name=\"Model\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_valid)\n",
    "    y_proba = model.predict_proba(X_valid)[:,1]\n",
    "    acc = accuracy_score(y_valid, y_pred)\n",
    "    auc = roc_auc_score(y_valid, y_proba)\n",
    "    print(f\"{name:25s} | Accuracy: {acc:.6f} | AUC: {auc:.6f}\")\n",
    "    return acc, auc\n",
    "\n",
    "# Logistic Regression (baseline)\n",
    "log_reg = LogisticRegression(max_iter=5000, solver='saga', C=0.5, n_jobs=-1)\n",
    "evaluate_model(log_reg, X_train, y_train, X_valid, y_valid, \"Logistic Regression\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=12,\n",
    "    min_samples_split=50,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "evaluate_model(rf, X_train, y_train, X_valid, y_valid, \"Random Forest\")\n",
    "\n",
    "# Gradient Boosting\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "evaluate_model(gb, X_train, y_train, X_valid, y_valid, \"Gradient Boosting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b046f1-5d00-41fd-b2af-3bda10104368",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform the data transformations required by a given algorithm.  For example, some algorithms require numeric data and perform better when it has been standardized or normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb862eb-97e4-4311-b95b-7f6a25f56080",
   "metadata": {},
   "source": [
    "## Using the downsampling and upsampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e99306fc-ce7c-4fc5-8770-abf615480c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (scaled)    -> Accuracy: 0.9194 | AUC: 0.7502\n",
      "Upsampled + Scaled   -> Accuracy: 0.6912 | AUC: 0.7502\n",
      "Downsampled + Scaled -> Accuracy: 0.6892 | AUC: 0.7483\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# --- Split train/valid ---\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# --- Upsampling ---\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_up, y_train_up = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# --- Downsampling ---\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_down, y_train_down = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# --- Define scaler ---\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# --- Baseline (no resampling, but scaled) ---\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=5000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "auc_base = roc_auc_score(y_valid, log_reg.predict_proba(X_valid_scaled)[:,1])\n",
    "acc_base = accuracy_score(y_valid, log_reg.predict(X_valid_scaled))\n",
    "\n",
    "# --- Upsampled + Scaled ---\n",
    "X_train_up_scaled = scaler.fit_transform(X_train_up)\n",
    "X_valid_scaled = scaler.transform(X_valid)   # always transform valid with same scaler\n",
    "\n",
    "log_reg.fit(X_train_up_scaled, y_train_up)\n",
    "auc_up = roc_auc_score(y_valid, log_reg.predict_proba(X_valid_scaled)[:,1])\n",
    "acc_up = accuracy_score(y_valid, log_reg.predict(X_valid_scaled))\n",
    "\n",
    "# --- Downsampled + Scaled ---\n",
    "X_train_down_scaled = scaler.fit_transform(X_train_down)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "\n",
    "log_reg.fit(X_train_down_scaled, y_train_down)\n",
    "auc_down = roc_auc_score(y_valid, log_reg.predict_proba(X_valid_scaled)[:,1])\n",
    "acc_down = accuracy_score(y_valid, log_reg.predict(X_valid_scaled))\n",
    "\n",
    "# --- Print results ---\n",
    "print(f\"Baseline (scaled)    -> Accuracy: {acc_base:.4f} | AUC: {auc_base:.4f}\")\n",
    "print(f\"Upsampled + Scaled   -> Accuracy: {acc_up:.4f} | AUC: {auc_up:.4f}\")\n",
    "print(f\"Downsampled + Scaled -> Accuracy: {acc_down:.4f} | AUC: {auc_down:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b943e3-4b18-4f9b-987c-416139ba7fe9",
   "metadata": {},
   "source": [
    "## Using the Ensemble method to find AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c4e778-8f0c-441f-8ba0-a3594c811557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble AUC: 0.7570\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define base models\n",
    "log_reg = LogisticRegression(max_iter=5000, class_weight='balanced')\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "gb = GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# Voting ensemble (soft = average predicted probabilities)\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('lr', log_reg), ('rf', rf), ('gb', gb)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_proba = ensemble.predict_proba(X_valid)[:,1]\n",
    "auc_ensemble = roc_auc_score(y_valid, y_proba)\n",
    "\n",
    "print(f\"Ensemble AUC: {auc_ensemble:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6588fd-04f8-4ab8-b8f5-51797c1a9551",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
